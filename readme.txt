[20250925: remove classes. add dynamic function call in /transform. add json transformation]
LargeScaleDBMigration class no longer needed to organized config parameters

[20250905: solved missing config files when running in cluster by create bash script to cp config files to cluster shared application directory and run docker exec cmd]
HOME=~/workspace/LargeScaleDBMigration
TABLE_NAME=supplier

cd $HOME

cp LargeScaleDBMigration.py ../../spark/spark-docker/spark_apps
cp config/${TABLE_NAME}.ini ../../spark/spark-docker/spark_apps/config
cp sql/${TABLE_NAME}.* ../../spark/spark-docker/spark_apps/sql

docker exec da-spark-master spark-submit --py-files apps/config/${TABLE_NAME}.ini,apps/sql/${TABLE_NAME}.staging.sql,apps/sql/${TABLE_NAME}.target.mysql.sql --master spark://localhost:7077 apps/LargeScaleDBMigration.py -t ${TABLE_NAME}

[20250901: add parameterization /sql and /config]

[20250828: solved jars problem when running application in spark cluster(local docker)]

[20250718: finalized gemini prompt]

These code generated by gemini from prompt below:

prompt#1
task:
write python program to migrate data from source to target database 

infrasturcture:
1. source machine host postgresql database(ip to be determine)
2. target machine host mysql database(ip to be determine)
3. etl machine to host postgresql program (ip to be determine)

requirement:
-able to handle 2,000,000,000 rows wide data ~ 20TB
-data tranformations expected(filter, data type and data code tranformations). source table might expands to multiple target tables or vice versa: multiple source tables might be sourced for a target table. there will be some xml and json transformation  
-able to chunck data for example chuck_size = 1,000,000
-able to throttle data especially when writting target database 
-preferably program does not write any data other than target database (but if chunk and throttling data require temporary data is acceptable)

constrain:
source machine, target machine are production machine and etl machine expected to have RAM constraint therefore chunking and throttling is essential

other:
please consider which python library work best. this should include and not limited to pandas, pyspark, dash etc.
