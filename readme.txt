These code generated by gemini from prompt below:

prompt#1
task:
write python program to migrate data from source to target database 

infrasturcture:
1. source machine host postgresql database(ip to be determine)
2. target machine host mysql database(ip to be determine)
3. etl machine to host postgresql program (ip to be determine)

requirement:
-able to handle 2,000,000,000 rows wide data ~ 20TB
-data tranformations expected(filter, data type and data code tranformations). source table might expands to multiple target tables or vice versa: multiple source tables might be sourced for a target table. there will be some xml and json transformation  
-able to chunck data for example chuck_size = 1,000,000
-able to throttle data especially when writting target database 
-preferably program does not write any data other than target database (but if chunk and throttling data require temporary data is acceptable)

constrain:
source machine, target machine are production machine and etl machine expected to have RAM constraint therefore chunking and throttling is essential

other:
please consider which python library work best. this should include and not limited to pandas, pyspark, dash etc.
